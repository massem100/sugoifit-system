{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd06c7349ce31c23824139823a5b551373d3093e2b1a419304cd56715663ebdff41",
   "display_name": "Python 3.9.2 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Machine', 'Learning', 'is', 'a', 'technique', 'of', 'parsing', 'data,', 'learn', 'from', 'that', 'data', 'and', 'then', 'apply', 'what', 'is', 'learned', 'to', 'make', 'an', 'informed', 'decision.', 'Machine', 'learning', 'focuses', 'on', 'designing', 'algorithms', 'that', 'can', 'learn', 'from', 'and', 'make', 'predictions', 'on', 'the', 'data.', 'The', 'learning', 'can', 'be', 'supervised', 'or', 'unsupervised.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what is learned to make an informed decision. Machine learning focuses on \n",
    "designing algorithms that can learn from and make predictions on the data. \n",
    "The learning can be supervised or unsupervised.\n",
    "\"\"\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word Count:\n Counter({'Machine': 2, 'is': 2, 'learn': 2, 'from': 2, 'that': 2, 'and': 2, 'make': 2, 'learning': 2, 'on': 2, 'can': 2, 'Learning': 1, 'a': 1, 'technique': 1, 'of': 1, 'parsing': 1, 'data,': 1, 'data': 1, 'then': 1, 'apply': 1, 'what': 1, 'learned': 1, 'to': 1, 'an': 1, 'informed': 1, 'decision.': 1, 'focuses': 1, 'designing': 1, 'algorithms': 1, 'predictions': 1, 'the': 1, 'data.': 1, 'The': 1, 'be': 1, 'supervised': 1, 'or': 1, 'unsupervised.': 1})\n\n Top 10 Words:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Machine', 2),\n",
       " ('is', 2),\n",
       " ('learn', 2),\n",
       " ('from', 2),\n",
       " ('that', 2),\n",
       " ('and', 2),\n",
       " ('make', 2),\n",
       " ('learning', 2),\n",
       " ('on', 2),\n",
       " ('can', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import collections as cl\n",
    "\n",
    "# Tokenize and create counter\n",
    "words = text.split()\n",
    "wc = cl.Counter(words)\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "machine learning is a technique of parsing data  learn from that data and then  apply what is learned to make an informed decision  machine learning focuses on  designing algorithms that can learn from and make predictions on the data   the learning can be supervised or unsupervised  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text_ns = re.sub(r'[^\\w]',' ', text).lower()\n",
    "print(text_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word Count:\n Counter({'learning': 3, 'data': 3, 'machine': 2, 'is': 2, 'learn': 2, 'from': 2, 'that': 2, 'and': 2, 'make': 2, 'on': 2, 'can': 2, 'the': 2, 'a': 1, 'technique': 1, 'of': 1, 'parsing': 1, 'then': 1, 'apply': 1, 'what': 1, 'learned': 1, 'to': 1, 'an': 1, 'informed': 1, 'decision': 1, 'focuses': 1, 'designing': 1, 'algorithms': 1, 'predictions': 1, 'be': 1, 'supervised': 1, 'or': 1, 'unsupervised': 1})\n\n Top 10 Words:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('learning', 3),\n",
       " ('data', 3),\n",
       " ('machine', 2),\n",
       " ('is', 2),\n",
       " ('learn', 2),\n",
       " ('from', 2),\n",
       " ('that', 2),\n",
       " ('and', 2),\n",
       " ('make', 2),\n",
       " ('on', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import collections as cl\n",
    "\n",
    "# Tokenize and create counter\n",
    "# print(text_ns)\n",
    "wc = cl.Counter(text_ns.split())\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word Count:\n Counter({'learning': 3, 'data': 3, 'machine': 2, 'learn': 2, 'make': 2, 'technique': 1, 'parsing': 1, 'apply': 1, 'learned': 1, 'informed': 1, 'decision': 1, 'focuses': 1, 'designing': 1, 'algorithms': 1, 'predictions': 1, 'supervised': 1, 'unsupervised': 1})\n\n Top 10 Words:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('learning', 3),\n",
       " ('data', 3),\n",
       " ('machine', 2),\n",
       " ('learn', 2),\n",
       " ('make', 2),\n",
       " ('technique', 1),\n",
       " ('parsing', 1),\n",
       " ('apply', 1),\n",
       " ('learned', 1),\n",
       " ('informed', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#remove stop words \n",
    "words = text_ns.split()\n",
    "words_no_stop = []\n",
    "for word in words: \n",
    "    if word not in stop_words: \n",
    "        words_no_stop.append(word)\n",
    "wc = cl.Counter(words_no_stop)\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word Count:\n Counter({'learn': 6, 'data': 3, 'machin': 2, 'make': 2, 'techniqu': 1, 'pars': 1, 'appli': 1, 'inform': 1, 'decis': 1, 'focus': 1, 'design': 1, 'algorithm': 1, 'predict': 1, 'supervis': 1, 'unsupervis': 1})\n\n Top 10 Words:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('learn', 6),\n",
       " ('data', 3),\n",
       " ('machin', 2),\n",
       " ('make', 2),\n",
       " ('techniqu', 1),\n",
       " ('pars', 1),\n",
       " ('appli', 1),\n",
       " ('inform', 1),\n",
       " ('decis', 1),\n",
       " ('focus', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer() \n",
    "words_clean = [] \n",
    "for word in words_no_stop: \n",
    "    words_clean.append(st.stem(word))\n",
    "\n",
    "wc = cl.Counter(words_clean)\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)   \n"
   ]
  },
  {
   "source": [
    "#Bag of Words\n",
    "##A simple question about text data mining that you might have is How can we classify documents made up of words when machine learning algorithms work on numerical data? The answer is simple. We need to build a numerical summary of a text data set that our algorithms can manipulate. A conceptually easy approach to implement this idea is to identify all possible words in the documents of interest and track the number of times each word occurs in specific documents. This produces a (very) sparse matrix for our sample of documents, where the columns are the possible words (or tokens) and the rows are different documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Tokens = 38\nTokens:\n['additional', 'algorithms', 'apply', 'appropriate', 'behavioral', 'catering', 'children', 'data', 'decision', 'designed', 'designing', 'difficulties', 'disabilities', 'education', 'educational', 'focuses', 'informed', 'learn', 'learned', 'learning', 'machine', 'make', 'needs', 'parsing', 'physical', 'predictions', 'problems', 'provide', 'resourced', 'school', 'schools', 'special', 'specifically', 'staffed', 'students', 'supervised', 'technique', 'unsupervised']\n----------------------------------------------------------------------------------------------------\nNumber of Training Samples = 2\nNumber of Testing Samples = 2\n----------------------------------------------------------------------------------------------------\nTesting dtm\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n  0 1]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0\n  0 0]]\n----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#training corpus\n",
    "training_text = [\"\"\"Machine Learning is a technique of parsing data, learn from that data and then \n",
    "        apply what is learned to make an informed decision. Machine learning focuses on \n",
    "        designing algorithms that can learn from and make predictions on the data. \n",
    "        The learning can be supervised or unsupervised.\"\"\",\n",
    "        \"\"\"A special school is a school catering for students who have special educational needs \n",
    "        due to learning difficulties, physical disabilities or behavioral problems. Special \n",
    "        schools may be specifically designed, staffed and resourced to provide appropriate \n",
    "        special education for children with additional needs.\n",
    "        \"\"\"]\n",
    "#testing corpus\n",
    "testing_text = [\"Machine learning has supervised and unsupervised learning.\",\n",
    "               \"Special education is important because children with special needs have equal rights to education.\"]\n",
    "\n",
    "# Define our vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Build a vocabulary from our training texts\n",
    "cv.fit(training_text)\n",
    "\n",
    "#transform training texts to a Document Term Matrix (DTM)\n",
    "training_dtm = cv.transform(training_text)\n",
    "\n",
    "# fitting and transforming could be done in one step \n",
    "# Build a vocabulary from our training text and transform training text\n",
    "# training_dtm = cv.fit_transform(training_text)\n",
    "\n",
    "#transform testing texts to a DTM\n",
    "testing_dtm = cv.transform(testing_text)\n",
    "\n",
    "#explore the characteristics of the matrix\n",
    "print(f'Number of Tokens = {training_dtm.shape[1]}')\n",
    "print('Tokens:')\n",
    "print(cv.get_feature_names())\n",
    "print(100*'-')\n",
    "print(f'Number of Training Samples = {training_dtm.shape[0]}')\n",
    "print(f'Number of Testing Samples = {training_dtm.shape[0]}')\n",
    "\n",
    "# print out testing DTM\n",
    "print(100*'-')\n",
    "print('Testing dtm')\n",
    "print(testing_dtm.todense())\n",
    "print(100*'-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training DTM: \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   additional  algorithms  apply  appropriate  behavioral  catering  children  \\\n",
       "0           0           1      1            0           0         0         0   \n",
       "1           1           0      0            1           1         1         1   \n",
       "\n",
       "   data  decision  designed  designing  difficulties  disabilities  education  \\\n",
       "0     3         1         0          1             0             0          0   \n",
       "1     0         0         1          0             1             1          1   \n",
       "\n",
       "   educational  focuses  informed  learn  learned  learning  machine  make  \\\n",
       "0            0        1         1      2        1         3        2     2   \n",
       "1            1        0         0      0        0         1        0     0   \n",
       "\n",
       "   needs  parsing  physical  predictions  problems  provide  resourced  \\\n",
       "0      0        1         0            1         0        0          0   \n",
       "1      2        0         1            0         1        1          1   \n",
       "\n",
       "   school  schools  special  specifically  staffed  students  supervised  \\\n",
       "0       0        0        0             0        0         0           1   \n",
       "1       2        1        4             1        1         1           0   \n",
       "\n",
       "   technique  unsupervised  \n",
       "0          1             1  \n",
       "1          0             0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>additional</th>\n      <th>algorithms</th>\n      <th>apply</th>\n      <th>appropriate</th>\n      <th>behavioral</th>\n      <th>catering</th>\n      <th>children</th>\n      <th>data</th>\n      <th>decision</th>\n      <th>designed</th>\n      <th>designing</th>\n      <th>difficulties</th>\n      <th>disabilities</th>\n      <th>education</th>\n      <th>educational</th>\n      <th>focuses</th>\n      <th>informed</th>\n      <th>learn</th>\n      <th>learned</th>\n      <th>learning</th>\n      <th>machine</th>\n      <th>make</th>\n      <th>needs</th>\n      <th>parsing</th>\n      <th>physical</th>\n      <th>predictions</th>\n      <th>problems</th>\n      <th>provide</th>\n      <th>resourced</th>\n      <th>school</th>\n      <th>schools</th>\n      <th>special</th>\n      <th>specifically</th>\n      <th>staffed</th>\n      <th>students</th>\n      <th>supervised</th>\n      <th>technique</th>\n      <th>unsupervised</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# set option to display all columns of data frame\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# create dataframe for training DTM \n",
    "training_data = training_dtm.todense().tolist()\n",
    "training_df = pd.DataFrame(training_data, columns = cv.get_feature_names())\n",
    "print('Training DTM: ')\n",
    "training_df"
   ]
  },
  {
   "source": [
    "TF-IDF\n",
    "Previously, we have simply used counts of tokens. Even with the removal of stop words, however, this can still overemphasize tokens that might generally occur across many documents (e.g., names or general concepts). An alternative technique that often provides robust improvements in text analysis accuracy is to employ the frequency of token occurrence, normalized over the frequency with which the token occurs in all documents. In this manner, we give higher weight in the text analysis process to tokens that are more strongly tied to a particular label(ie. topic).\n",
    "\n",
    "Formally this concept is known as term frequencyâ€“inverse document frequency (or tf-idf), and scikit-learn provides this functionality via the TfidfTransformer that can either follow a tokenizer, such as CountVectorizer or can be combined together into a single transformer via the TfidfVectorizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Define our TF-IDF Vectoriizer \n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Buildd vocabulary from our training text and transform training text \n",
    "training_dtm_tf = tf_cv.fit_transform(training_text)\n",
    "\n",
    "# Transform testing set \n",
    "testing_dtm_tf = tf_cv.transform(testing_text)\n",
    "\n"
   ]
  }
 ]
}