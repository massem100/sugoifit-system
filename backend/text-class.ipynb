{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd06c7349ce31c23824139823a5b551373d3093e2b1a419304cd56715663ebdff41",
   "display_name": "Python 3.9.2 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "729cc854aa18b1afaaeb0aa5c762d0a3c3b18ba86d0b5156dfab5f262336a499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-222001717ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create DTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtest_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Classify text with Naive Bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create DTM\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_dtm = cv.fit_transform(train['data'])\n",
    "test_dtm = cv.transform(test['data'])\n",
    "\n",
    "# Create Classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, train['target'])\n",
    "\n",
    "# Predict and display score\n",
    "predicted = nb.predict(test_dtm)\n",
    "score = nb.score(test_dtm, test['target'])\n",
    "print(f'NB prediction accuracy = {score:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "svc = LinearSVC(C=1000)\n",
    "\n",
    "svc.fit(train_dtm_df, train['target'])\n",
    "predicted = svc.predict(test_dtm_tf)\n",
    "\n",
    "score = svc.score(test_dtm_df, test['target'])\n",
    "print(f'SVC (TF-IDF with Stop Words) prediction accuracy = {score:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out results\n",
    "print(metrics.classification_report(test['target'], \n",
    "                                    predicted,\n",
    "                                    target_names = test['target_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr.fit(train_dtm_tf, train['target'])\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "score = lr.score(test_dtm_tf, test['target'])\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {score:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 important words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add com, edu and re to stop words\n",
    "stop_words.extend(['com', 'edu', 're'])\n",
    "\n",
    "# Create DTM, use custom defined stop words\n",
    "tf_cv = TfidfVectorizer(stop_words=stop_words)\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "score = nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {score:.1%}')"
   ]
  },
  {
   "source": [
    "# NGRAMS\n",
    "\n",
    "#### a n-gram is a contiguous sequence of n items from a parent sequence of items, such as characters or words in a text document. \n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = 'This course introduces many concepts in data science.'\n",
    "\n",
    "# Tokenize Sentence \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "cv = CountVectorizer(ngram_range(1,3))\n",
    "\n",
    "# Analyze Sentence \n",
    "tk_func = cv.build_analyzer() \n",
    "\n",
    "# Display n-grams \n",
    "print(tk_func(my_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokensize Sentence \n",
    "cv.fit([my_text])\n",
    "\n",
    "# Sort tokens\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key = operator.itemgetter)\n",
    "\n",
    "#Display token mapping \n",
    "print('Token mapping: ')\n",
    "print (40*'-')\n",
    "\n",
    "for tokens, rank in my_voc: \n",
    "    print(rank, tokens)\n",
    "\n",
    "# Display new sentence\n",
    "print(40*'-')\n",
    "out_list = ['This course is data science!']\n",
    "\n",
    "# Transform new sentence to original sentence DTM\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "\n",
    "# Display count vector indices for new sentecne tokens \n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "source": [
    "# Stemming\n",
    "\n",
    "#### In the previous lesson, we introduced the concept of stemming. In the next Code cell, we demonstrate how to apply stemming in text classification. We will use PorterStemmer in the nltk module for stemming.\n",
    "\n",
    "#### We first define a function tokenize. The function has one argument text which is the text to be tokenized. The function uses nltk.word_tokenize function to tokenize text then apply PorterStemmer to stem the tokens. We then set tokenizer argument in CounterVectorizer or TfidfVectorizer with this custom tokenize function and use the new vectorizer to create bag of words.\n",
    "\n",
    "#### The following code takes longer to finish due to stemming, but it does give a better classification accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Define function to tokenize text and apply stemmer\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "# use custom tokenize when creating vectorizer\n",
    "tf_cv = TfidfVectorizer(tokenizer=tokenize)\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "score = lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stemming) prediction accuracy = {score:.1%}')\n"
   ]
  }
 ]
}